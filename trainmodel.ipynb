{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "dbbe07da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras_preprocessing in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras_preprocessing) (1.24.3)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras_preprocessing) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras_preprocessing\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.image import load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "abd22b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'images'\n",
    "TEST_DIR = 'images'\n",
    "#TEST_DIR = 'images/C:\\Users\\user\\Desktop\\AI\\codecluase ai\\facial\\archive\\.ipynb_checkpoints\\test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "124913d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createdataframe(dir):\n",
    "    image_paths=[]\n",
    "    labels=[]\n",
    "    for label in os.listdir(dir):\n",
    "        for imagename in os.listdir(os.path.join(dir, label)):\n",
    "            image_paths.append(os.path.join(dir, label,imagename))\n",
    "            labels.append(label)\n",
    "        print(label, \"completed\")\n",
    "    return image_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "6cdb36cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 completed\n",
      "1 completed\n",
      "10 completed\n",
      "11 completed\n",
      "12 completed\n",
      "13 completed\n",
      "14 completed\n",
      "15 completed\n",
      "16 completed\n",
      "17 completed\n",
      "18 completed\n",
      "2 completed\n",
      "5 completed\n",
      "6 completed\n",
      "7 completed\n",
      "8 completed\n"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame()\n",
    "train['image'],train['label'] = createdataframe(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "44fbe4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      image label\n",
      "0        images\\0\\Anger.jpg     0\n",
      "1     images\\0\\Contempt.jpg     0\n",
      "2      images\\0\\Disgust.jpg     0\n",
      "3         images\\0\\Fear.jpg     0\n",
      "4        images\\0\\Happy.jpg     0\n",
      "..                      ...   ...\n",
      "115       images\\8\\Fear.jpg     8\n",
      "116      images\\8\\Happy.jpg     8\n",
      "117    images\\8\\Neutral.jpg     8\n",
      "118        images\\8\\Sad.jpg     8\n",
      "119  images\\8\\Surprised.jpg     8\n",
      "\n",
      "[120 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "77ef0cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 completed\n",
      "1 completed\n",
      "10 completed\n",
      "11 completed\n",
      "12 completed\n",
      "13 completed\n",
      "14 completed\n",
      "15 completed\n",
      "16 completed\n",
      "17 completed\n",
      "18 completed\n",
      "2 completed\n",
      "5 completed\n",
      "6 completed\n",
      "7 completed\n",
      "8 completed\n"
     ]
    }
   ],
   "source": [
    "test = pd.DataFrame()\n",
    "test['image'],test['label'] = createdataframe(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "86489cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      image label\n",
      "0        images\\0\\Anger.jpg     0\n",
      "1     images\\0\\Contempt.jpg     0\n",
      "2      images\\0\\Disgust.jpg     0\n",
      "3         images\\0\\Fear.jpg     0\n",
      "4        images\\0\\Happy.jpg     0\n",
      "..                      ...   ...\n",
      "115       images\\8\\Fear.jpg     8\n",
      "116      images\\8\\Happy.jpg     8\n",
      "117    images\\8\\Neutral.jpg     8\n",
      "118        images\\8\\Sad.jpg     8\n",
      "119  images\\8\\Surprised.jpg     8\n",
      "\n",
      "[120 rows x 2 columns]\n",
      "0          images\\0\\Anger.jpg\n",
      "1       images\\0\\Contempt.jpg\n",
      "2        images\\0\\Disgust.jpg\n",
      "3           images\\0\\Fear.jpg\n",
      "4          images\\0\\Happy.jpg\n",
      "                ...          \n",
      "115         images\\8\\Fear.jpg\n",
      "116        images\\8\\Happy.jpg\n",
      "117      images\\8\\Neutral.jpg\n",
      "118          images\\8\\Sad.jpg\n",
      "119    images\\8\\Surprised.jpg\n",
      "Name: image, Length: 120, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "88f848cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "f1bd2323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a066b2c8dae14a82a2f6206e598b3bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c477efe38644ca9a0e6f5b239e8e703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def extract_features(images):\n",
    "    features = []\n",
    "    for image in tqdm(images):\n",
    "        img = load_img(image, grayscale=True)  # Load image\n",
    "        img = img.resize((width, height))      # Resize image to a common size\n",
    "        img = np.array(img)                    # Convert image to numpy array\n",
    "        features.append(img)\n",
    "    features = np.array(features)\n",
    "    return features\n",
    "\n",
    "# Define target width and height for resizing\n",
    "width = 48\n",
    "height = 48\n",
    "\n",
    "# Load and extract features\n",
    "train_features = extract_features(train['image'])\n",
    "test_features = extract_features(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "9a07e6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0d6a412f0c4a2a81b0b5e4dc1d501a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_features = extract_features(train['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "d312b90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10328a7fb3754af98a59a5e40c86d84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_features = extract_features(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "18b5cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_features/255.0\n",
    "x_test = train_features/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "ac4e7587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "675cfbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "976f8fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = le.transform(train['label'])\n",
    "y_test = le.transform(test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "db8de761",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train,num_classes = 0)\n",
    "y_test = to_categorical(y_test,num_classes = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "96b6a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# Flatten layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "# Print model summary\n",
    "#model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "1296aeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "5627bf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 15) and (None, 7) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[303], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x\u001b[38;5;241m=\u001b[39mx_train,y\u001b[38;5;241m=\u001b[39my_train, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m, epochs \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, validation_data \u001b[38;5;241m=\u001b[39m (x_test,y_test))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filegv6juw82.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 15) and (None, 7) are incompatible\n"
     ]
    }
   ],
   "source": [
    "model.fit(x=x_train,y=y_train, batch_size = 128, epochs =100, validation_data = (x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "4b795785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 15) and (None, 48, 48) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[375], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Fit the model with your training data\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(x\u001b[38;5;241m=\u001b[39mx_train, y\u001b[38;5;241m=\u001b[39my_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(x_test, y_test))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filegv6juw82.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 15) and (None, 48, 48) are incompatible\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add layers to your model...\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model with your training data\n",
    "history = model.fit(x=x_train, y=y_train, batch_size=128, epochs=100, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "e9a5325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"emotionaldetector.json\",'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"emotionaldetector.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "ea05715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "7ae5b5d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[384], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model_json \u001b[38;5;241m=\u001b[39m json_file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      3\u001b[0m json_file\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m model_from_json(model_json)\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacialemotionalmodel.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\legacy\\model_config.py:125\u001b[0m, in \u001b[0;36mmodel_from_json\u001b[1;34m(json_string, custom_objects)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parses a JSON model configuration string and returns a model instance.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03mUsage:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    A Keras model instance (uncompiled).\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    122\u001b[0m     deserialize_from_json,\n\u001b[0;32m    123\u001b[0m )\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m deserialize_from_json(json_string, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\serialization.py:294\u001b[0m, in \u001b[0;36mdeserialize_from_json\u001b[1;34m(json_string, custom_objects)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Instantiates a layer from a JSON string.\"\"\"\u001b[39;00m\n\u001b[0;32m    293\u001b[0m populate_deserializable_objects()\n\u001b[1;32m--> 294\u001b[0m config \u001b[38;5;241m=\u001b[39m json_utils\u001b[38;5;241m.\u001b[39mdecode_and_deserialize(\n\u001b[0;32m    295\u001b[0m     json_string,\n\u001b[0;32m    296\u001b[0m     module_objects\u001b[38;5;241m=\u001b[39mLOCAL\u001b[38;5;241m.\u001b[39mALL_OBJECTS,\n\u001b[0;32m    297\u001b[0m     custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    298\u001b[0m )\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m deserialize(config, custom_objects)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\json_utils.py:80\u001b[0m, in \u001b[0;36mdecode_and_deserialize\u001b[1;34m(json_string, module_objects, custom_objects)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_and_deserialize\u001b[39m(\n\u001b[0;32m     77\u001b[0m     json_string, module_objects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, custom_objects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     78\u001b[0m ):\n\u001b[0;32m     79\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decodes the JSON and deserializes any Keras objects found in the dict.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(\n\u001b[0;32m     81\u001b[0m         json_string,\n\u001b[0;32m     82\u001b[0m         object_hook\u001b[38;5;241m=\u001b[39mfunctools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m     83\u001b[0m             _decode_helper,\n\u001b[0;32m     84\u001b[0m             deserialize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     85\u001b[0m             module_objects\u001b[38;5;241m=\u001b[39mmodule_objects,\n\u001b[0;32m     86\u001b[0m             custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m     87\u001b[0m         ),\n\u001b[0;32m     88\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\json\\__init__.py:359\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m     kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_constant\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m parse_constant\n\u001b[1;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39mdecode(s)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "json_file = open(r'C:\\Users\\user\\Desktop\\AI\\codecluase ai\\waste\\facialemotionalmodel.json', \"r\")\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights(\"facialemotionalmodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "c76564a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['disgust','happy','anger','neutral','fear','contempt','sad','surprised']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "dee68337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ef(image):\n",
    "    img = load_img(image, grayscale=True)\n",
    "    feature = np.array(img)\n",
    "    feature.reshape(1,48,48,1)\n",
    "    return feature/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "c5c07d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of angry\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 20113920 into shape (1,48,48,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[387], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAI\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAnger\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAnger (2).jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal image is of angry\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m img \u001b[38;5;241m=\u001b[39m ef(image)\n\u001b[0;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img)\n\u001b[0;32m      5\u001b[0m pred_label \u001b[38;5;241m=\u001b[39m label[pred\u001b[38;5;241m.\u001b[39margmax()]\n",
      "Cell \u001b[1;32mIn[386], line 4\u001b[0m, in \u001b[0;36mef\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      2\u001b[0m img \u001b[38;5;241m=\u001b[39m load_img(image, grayscale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m feature \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img)\n\u001b[1;32m----> 4\u001b[0m feature\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m48\u001b[39m,\u001b[38;5;241m48\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m feature\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.0\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 20113920 into shape (1,48,48,1)"
     ]
    }
   ],
   "source": [
    "image = r'C:\\Users\\user\\Desktop\\AI\\images\\Anger\\Anger (2).jpg'\n",
    "print(\"original image is of angry\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \", pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "c0b2bf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of angry\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def ef(image_path):\n",
    "    img = load_img(image_path, grayscale=True)\n",
    "    img = img.resize((48, 48))  # Resize the image to (48, 48)\n",
    "    feature = np.array(img)\n",
    "    feature = feature.reshape(1, 48, 48, 1)  # Reshape the array\n",
    "    return feature / 255.0\n",
    "\n",
    "# Example usage:\n",
    "image = r'C:\\Users\\user\\Desktop\\AI\\images\\Sad\\Sad (4).jpg'\n",
    "print(\"original image is of angry\")\n",
    "img = ef(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "08c68c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[389], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img)\n\u001b[1;32m----> 2\u001b[0m pred_label \u001b[38;5;241m=\u001b[39m label[pred\u001b[38;5;241m.\u001b[39margmax()]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel prediction is \u001b[39m\u001b[38;5;124m\"\u001b[39m, pred_label)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \", pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "9324679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "681f7173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of angry\n",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[391], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m img \u001b[38;5;241m=\u001b[39m ef(image)\n\u001b[0;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img)\n\u001b[1;32m----> 5\u001b[0m pred_label \u001b[38;5;241m=\u001b[39m label[pred\u001b[38;5;241m.\u001b[39margmax()]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel prediction is \u001b[39m\u001b[38;5;124m\"\u001b[39m, pred_label)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(img\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m48\u001b[39m,\u001b[38;5;241m48\u001b[39m),cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "image = r'C:\\Users\\user\\Desktop\\AI\\images\\Sad\\Sad (4).jpg'\n",
    "print(\"original image is of angry\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \", pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "a5a9b0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of angry\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[392], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m img \u001b[38;5;241m=\u001b[39m ef(image)\n\u001b[0;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img)\n\u001b[1;32m----> 5\u001b[0m pred_label \u001b[38;5;241m=\u001b[39m label[pred\u001b[38;5;241m.\u001b[39margmax()]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel prediction is \u001b[39m\u001b[38;5;124m\"\u001b[39m, pred_label)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(img\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m48\u001b[39m,\u001b[38;5;241m48\u001b[39m),cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "image = r'C:\\Users\\user\\Desktop\\AI\\images\\happy\\Happy (2).jpg'\n",
    "print(\"original image is of angry\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \", pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "a2f79c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of angry\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\user\\\\Desktop\\\\AI\\\\images\\\\Anger\\\\Anger (1).jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[393], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAI\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAnger\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAnger (1).jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal image is of angry\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m img \u001b[38;5;241m=\u001b[39m ef(image)\n\u001b[0;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img)\n\u001b[0;32m      5\u001b[0m pred_label \u001b[38;5;241m=\u001b[39m label[pred\u001b[38;5;241m.\u001b[39margmax()]\n",
      "Cell \u001b[1;32mIn[388], line 6\u001b[0m, in \u001b[0;36mef\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mef\u001b[39m(image_path):\n\u001b[1;32m----> 6\u001b[0m     img \u001b[38;5;241m=\u001b[39m load_img(image_path, grayscale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m48\u001b[39m, \u001b[38;5;241m48\u001b[39m))  \u001b[38;5;66;03m# Resize the image to (48, 48)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     feature \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\image_utils.py:422\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, pathlib\u001b[38;5;241m.\u001b[39mPath):\n\u001b[0;32m    421\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    423\u001b[0m         img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread()))\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\user\\\\Desktop\\\\AI\\\\images\\\\Anger\\\\Anger (1).jpg'"
     ]
    }
   ],
   "source": [
    "image = r'C:\\Users\\user\\Desktop\\AI\\images\\Anger\\Anger (1).jpg'\n",
    "print(\"original image is of angry\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \", pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "49ce465e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of angry\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[358], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m img \u001b[38;5;241m=\u001b[39m ef(image)\n\u001b[0;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img)\n\u001b[1;32m----> 5\u001b[0m pred_label \u001b[38;5;241m=\u001b[39m label[pred\u001b[38;5;241m.\u001b[39margmax()]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel prediction is \u001b[39m\u001b[38;5;124m\"\u001b[39m, pred_label)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(img\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m48\u001b[39m,\u001b[38;5;241m48\u001b[39m),cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "image = r'C:\\Users\\user\\Desktop\\AI\\images\\Sad\\Sad (2).jpg'\n",
    "print(\"original image is of angry\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \", pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "3f7bad03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of angry\n",
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[359], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m img \u001b[38;5;241m=\u001b[39m ef(image)\n\u001b[0;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img)\n\u001b[1;32m----> 5\u001b[0m pred_label \u001b[38;5;241m=\u001b[39m label[pred\u001b[38;5;241m.\u001b[39margmax()]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel prediction is \u001b[39m\u001b[38;5;124m\"\u001b[39m, pred_label)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(img\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m48\u001b[39m,\u001b[38;5;241m48\u001b[39m),cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "image = r'C:\\Users\\user\\Desktop\\AI\\images\\Anger\\Anger (2).jpg'\n",
    "print(\"original image is of angry\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \", pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "999f6ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of angry\n",
      "1/1 [==============================] - 0s 36ms/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[319], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m img \u001b[38;5;241m=\u001b[39m ef(image)\n\u001b[0;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img)\n\u001b[1;32m----> 5\u001b[0m pred_label \u001b[38;5;241m=\u001b[39m label[pred\u001b[38;5;241m.\u001b[39margmax()]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel prediction is \u001b[39m\u001b[38;5;124m\"\u001b[39m, pred_label)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(img\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m48\u001b[39m,\u001b[38;5;241m48\u001b[39m),cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "image = r'C:\\Users\\user\\Desktop\\AI\\images\\Disgust\\Disgust (3).jpg'\n",
    "print(\"original image is of angry\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \", pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "344adbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of angry\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[324], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m img \u001b[38;5;241m=\u001b[39m ef(image)\n\u001b[0;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img)\n\u001b[1;32m----> 5\u001b[0m pred_label \u001b[38;5;241m=\u001b[39m label[pred\u001b[38;5;241m.\u001b[39margmax()]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel prediction is \u001b[39m\u001b[38;5;124m\"\u001b[39m, pred_label)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(img\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m48\u001b[39m,\u001b[38;5;241m48\u001b[39m),cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "image = r'C:\\Users\\user\\Desktop\\AI\\images\\Disgust\\Disgust (4).jpg'\n",
    "print(\"original image is of angry\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \", pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "9fdbd840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of angry\n",
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[321], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m img \u001b[38;5;241m=\u001b[39m ef(image)\n\u001b[0;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img)\n\u001b[1;32m----> 5\u001b[0m pred_label \u001b[38;5;241m=\u001b[39m label[pred\u001b[38;5;241m.\u001b[39margmax()]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel prediction is \u001b[39m\u001b[38;5;124m\"\u001b[39m, pred_label)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(img\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m48\u001b[39m,\u001b[38;5;241m48\u001b[39m),cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "image = r'C:\\Users\\user\\Desktop\\AI\\images\\fear\\fear (2).jpg'\n",
    "print(\"original image is of angry\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \", pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "2f51d0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of angry\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[322], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m img \u001b[38;5;241m=\u001b[39m ef(image)\n\u001b[0;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img)\n\u001b[1;32m----> 5\u001b[0m pred_label \u001b[38;5;241m=\u001b[39m label[pred\u001b[38;5;241m.\u001b[39margmax()]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel prediction is \u001b[39m\u001b[38;5;124m\"\u001b[39m, pred_label)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(img\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m48\u001b[39m,\u001b[38;5;241m48\u001b[39m),cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "image = r'C:\\Users\\user\\Desktop\\AI\\images\\Neutral\\Neutral (3).jpg'\n",
    "print(\"original image is of angry\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \", pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3672b366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc86e9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
